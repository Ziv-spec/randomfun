{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do some more preprocessing (so searching could get even better)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x000001C21B8B4708>, origin='e:\\\\Programs\\\\Anaconda\\\\envs\\\\net\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py', submodule_search_locations=['e:\\\\Programs\\\\Anaconda\\\\envs\\\\net\\\\lib\\\\site-packages\\\\numpy'])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.util.find_spec('numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_files(filenames):\n",
    "  documents = []\n",
    "  for f in filenames: \n",
    "    if f.endswith('.html'): \n",
    "      try: \n",
    "        with open(f, 'r', encoding='utf8') as html: \n",
    "            soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "            documents.append(' '.join([p.text for p in soup.find_all('p')]))\n",
    "      except: \n",
    "        continue\n",
    "    elif f.endswith('.pdf'): \n",
    "        doc = fitz.open(f)\n",
    "        documents.append(' '.join([p.get_text() for p in doc]))\n",
    "    else: \n",
    "      with open(f, 'r', encoding='utf8') as file: \n",
    "        documents.append(file.read())\n",
    "    # print(f)\n",
    "  return documents\n",
    "\n",
    "def walk(base, filter=lambda x: x.endswith('pdf') or x.endswith('html')):\n",
    "  files = []\n",
    "  for p, d, f in os.walk(base):\n",
    "    for file in f:\n",
    "      if filter(file):\n",
    "        files.append(os.path.join(p, file))\n",
    "  return files  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import array\n",
    "from scipy import sparse as sp\n",
    "\n",
    "# source: \"from nltk.corpus import stopwords; stopwords.words('english')\"\n",
    "stop_words = [ \n",
    "  'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    " \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
    " 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
    " \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
    " 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is',\n",
    " 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    " 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
    " 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    " 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    " 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n",
    " 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    " 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
    " 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\",\n",
    " 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
    " \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    " \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    " \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn',\n",
    " \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "]\n",
    "\n",
    "def tokenize(text):\n",
    "  return re.findall(r\"(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b|\\d+\", text.lower())\n",
    "\n",
    "def remove_stop_words(tokens, stop_wrods):\n",
    "  inconsistent = []\n",
    "  for token in tokens:\n",
    "      if token not in stop_wrods:\n",
    "        inconsistent.append(token)\n",
    "  return inconsistent\n",
    "\n",
    "_stopwords = list(np.unique(tokenize(' '.join(stop_words))))\n",
    "\n",
    "def vectorize(documents, vocabulary={}):\n",
    "  if len(vocabulary) == 0: \n",
    "    # Add a new value when a new vocabulary item is seen\n",
    "    vocabulary = defaultdict(None, vocabulary)\n",
    "    vocabulary.default_factory = vocabulary.__len__\n",
    "\n",
    "  j_indices = []\n",
    "  indptr = []\n",
    "\n",
    "  values = array.array(str(\"i\"))\n",
    "\n",
    "  indptr.append(0)\n",
    "  for i, doc in enumerate(documents): \n",
    "    tokens = tokenize(doc) # turn into tokens\n",
    "    tokens = remove_stop_words(tokens, stop_words)\n",
    "\n",
    "    feature_counter = defaultdict(lambda:0)\n",
    "    for token in tokens:\n",
    "      token_idx = vocabulary[token]\n",
    "      feature_counter[token_idx] += 1\n",
    "\n",
    "    j_indices.extend(feature_counter.keys())\n",
    "    values.extend(feature_counter.values())\n",
    "    indptr.append(len(j_indices))\n",
    "\n",
    "  vocabulary = dict(vocabulary) # disable default dict behaviour\n",
    "\n",
    "  indices_dtype = np.int32\n",
    "  j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
    "  indptr = np.asarray(indptr, dtype=indices_dtype)\n",
    "  values = np.frombuffer(values, dtype=np.intc)\n",
    "\n",
    "  X = sp.csr_matrix(\n",
    "      (values, j_indices, indptr),\n",
    "      shape=(len(indptr) - 1, len(vocabulary)),\n",
    "      dtype=np.float32\n",
    "  )\n",
    "\n",
    "  X.sort_indices()\n",
    "  return X, vocabulary\n",
    "\n",
    "def calc_tf_idf(X):\n",
    "  N = X.shape[0]\n",
    "  tf  = X / (X.sum(axis=1, keepdims=True)+0.1)\n",
    "  df  = (X > 0).sum(axis=0, keepdims=True)\n",
    "  idf = np.log10(N / (1+df))+1\n",
    "  return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cache_filename ='index_cache.npz'\n",
    "\n",
    "def load_cache():\n",
    "  global cache_filename\n",
    "  cache = np.load(cache_filename)\n",
    "  cache.allow_pickle=True\n",
    "\n",
    "  fts = dict(zip(cache['files'], cache['timestamps']))\n",
    "\n",
    "  v = {}\n",
    "  words = cache['vocab']\n",
    "  for i, word in enumerate(words): \n",
    "    v[word] = i\n",
    "\n",
    "  return fts, v, cache['matrix'] \n",
    "\n",
    "def save_cache(files, timestamps, vocabulary, matrix):\n",
    "  assert len(files) == len(timestamps)\n",
    "  np.savez_compressed(\n",
    "    cache_filename, \n",
    "    files=np.asarray(files),\n",
    "    timestamps=np.asarray(timestamps),\n",
    "    vocab=np.asarray(list(vocabulary.keys())), \n",
    "    matrix=matrix\n",
    "  )\n",
    "\n",
    "if not os.path.exists(cache_filename):\n",
    "  save_cache([], [], {}, np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_cache([], [], {}, np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts, v, m = load_cache()\n",
    "\n",
    "all_files = [] + \\\n",
    "  walk('C:\\\\Users\\\\bkand\\\\Downloads\\\\', lambda x: x.endswith('pdf')) + \\\n",
    "  walk('E:\\\\imp\\\\', lambda x: True) + \\\n",
    "  walk('E:\\\\archivebox\\\\archive\\\\', lambda x: x.endswith('.html') and x.find('.orig')==-1 and x.find('woff2')==-1)\n",
    "\n",
    "new_files = []\n",
    "modified_files = []\n",
    "for f in all_files: \n",
    "  timestamp = fts.get(f)\n",
    "  if timestamp: \n",
    "    if timestamp != os.path.getmtime(f):\n",
    "      modified_files.append(f)\n",
    "      print('mod', f)\n",
    "  else: \n",
    "    if not f in fts.keys():\n",
    "      print('new', f)\n",
    "      new_files.append(f)\n",
    "\n",
    "deleted_files = []\n",
    "for f in fts.keys():\n",
    "  if not f in all_files: \n",
    "    print('del', f)\n",
    "    deleted_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(fts.keys())\n",
    "\n",
    "if len(new_files) > 0:\n",
    "  # create count matrix for new files \n",
    "  new_documents = extract_text_from_files(new_files)\n",
    "  new_mat, new_vocabulary = vectorize(new_documents, vocabulary=v)\n",
    "\n",
    "  # update tf-idf matrix\n",
    "  old_mat = sp.csr_matrix(m)\n",
    "  old_mat.resize((old_mat.shape[0], new_mat.shape[1]))\n",
    "  count_matrix = sp.vstack([old_mat, sp.csr_matrix(new_mat)]) # merge count matricies\n",
    "  count_matrix = count_matrix.toarray()\n",
    "  tf_idf = calc_tf_idf(count_matrix)\n",
    "\n",
    "  # update vocabulary \n",
    "  new_idx = len(v.keys())\n",
    "  for word in new_vocabulary.keys(): \n",
    "    idx = v.get(word)\n",
    "    if idx is None:\n",
    "      v[word] = new_idx\n",
    "      new_idx += 1\n",
    "\n",
    "  # update file names\n",
    "  files += new_files\n",
    "\n",
    "if len(deleted_files) > 0: \n",
    "  # remove unwanted columns from old tf-idf matrix \n",
    "  old_mat = m\n",
    "  idxs = [files.index(df) for df in deleted_files]\n",
    "  count_matrix = np.delete(old_mat, idxs, axis=0)   \n",
    "  tf_idf = calc_tf_idf(count_matrix)\n",
    "\n",
    "  # update file names \n",
    "  for idx in idxs:\n",
    "    files.remove(files[idx])\n",
    "\n",
    "if len(modified_files) > 0:\n",
    "  # create count matrix for new files \n",
    "  mod_documents = extract_text_from_files(modified_files)\n",
    "  mod_mat, mod_vocabulary = vectorize(mod_documents, vocabulary=v)\n",
    "\n",
    "  # update tf-idf matrix\n",
    "  count_matrix = m\n",
    "  mod_mat = mod_mat.toarray()\n",
    "  idxs = [files.index(mf) for mf in modified_files]\n",
    "  for i, idx in enumerate(idxs):\n",
    "    count_matrix[idx] = mod_mat[i]\n",
    "  tf_idf = calc_tf_idf(count_matrix)\n",
    "\n",
    "  # update vocabulary \n",
    "  mod_idx = len(v.keys())\n",
    "  for word in mod_vocabulary.keys(): \n",
    "    idx = v.get(word)\n",
    "    if idx is None:\n",
    "      v[word] = mod_idx\n",
    "      mod_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_files + deleted_files + modified_files) > 0: \n",
    "  cache_filename ='index_cache.npz'\n",
    "  timestamps = [os.path.getmtime(f) for f in files]\n",
    "  save_cache(files, timestamps, v, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_query(X, query, vocabulary={}):\n",
    "\n",
    "  Q, _ = vectorize([query], vocabulary=vocabulary)\n",
    "  Q = calc_tf_idf(Q.toarray())\n",
    "\n",
    "  def cosine_sim(a, b):\n",
    "    d = np.linalg.norm(a)*np.linalg.norm(b)\n",
    "    cos_sim = np.dot(a, b)/d if d > 0 else 0\n",
    "    return cos_sim\n",
    "\n",
    "  sim = {}\n",
    "  # Calculate the similarity\n",
    "  for i in range(X.shape[0]):\n",
    "    sim[i] = cosine_sim(Q, X[i])\n",
    "\n",
    "  # Sort the values \n",
    "  sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # Print the articles and their similarity values\n",
    "  for k, v in sim_sorted:\n",
    "    if v != 0.0 and not np.isnan(v):\n",
    "      print(f\"Nilai Similaritas: {v.item():.4f} - {files[k].split('/')[-1]}\")\n",
    "\n",
    "query = 'memory'\n",
    "\n",
    "get_files_from_query(tf_idf, query, vocabulary=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.asarray([[0, 1], [2, 3]])\n",
    "arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[arr, np.array([0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_mat = np.ones((10, 100))\n",
    "new_mat = np.arange((10* 102)).reshape((10, 102))\n",
    "\n",
    "# old_mat.resize((old_mat.shape[0], new_mat.shape[1]))\n",
    "# count_matrix = np.vstack([old_mat, new_mat]) # merge count matricies\n",
    "# tf_idf = calc_tf_idf(count_matrix)\n",
    "# for l in old_mat:\n",
    "#   print(l)\n",
    "\n",
    "# old_mat.resize((old_mat.shape[0], new_mat.shape[1]))\n",
    "# count_matrix = np.vstack([old_mat, new_mat]) # merge count matricies\n",
    "# for l in count_matrix:\n",
    "#   print(l)\n",
    "c = np.c_[old_mat, np.zeros((old_mat.shape[0], new_mat.shape[1]-old_mat.shape[1]))]\n",
    "result = np.r_[c, new_mat]\n",
    "result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "def get_files():\n",
    "  all_files = []\n",
    "  all_files += [] + \\\n",
    "    walk('C:\\\\Users\\\\bkand\\\\Downloads\\\\', lambda x: x.endswith('pdf')) + \\\n",
    "    walk('E:\\\\imp\\\\', lambda x: True) + \\\n",
    "    walk('E:\\\\archivebox\\\\archive\\\\', lambda x: x.endswith('.html') and x.find('.orig')==-1 and x.find('woff2')==-1)\n",
    "  return all_files\n",
    "  \n",
    "def load_data():\n",
    "  global fts, v, m \n",
    "  fts, v, m = load_cache()\n",
    "\n",
    "start  = time.time()\n",
    "\n",
    "CACHE_FILENAME = 'index_cache.npz'\n",
    "\n",
    "class LoadFileNamesThread(Thread):\n",
    "  def __init__(self, roots_and_filters, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.roots_and_filters = roots_and_filters\n",
    "  def run(self):\n",
    "    self.all_files = []\n",
    "    for name, filter in self.roots_and_filters:\n",
    "      for p, d, f in os.walk(name):\n",
    "        for file in f:\n",
    "          if filter(file): \n",
    "            self.all_files.append(p + file)\n",
    "\n",
    "class LoadIndexThread(Thread):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "  def run(self):\n",
    "    cache = np.load(CACHE_FILENAME)\n",
    "    cache.allow_pickle=True\n",
    "\n",
    "    self.m = cache['matrix']\n",
    "    self.fts = dict(zip(cache['files'], cache['timestamps']))\n",
    "    self.v = {}\n",
    "    for i, word in enumerate(cache['vocab']): \n",
    "      self.v[word] = i\n",
    "\n",
    "def save_cache(files, timestamps, vocabulary, matrix):\n",
    "  assert len(files) == len(timestamps)\n",
    "  np.savez_compressed(CACHE_FILENAME, \n",
    "    files=np.asarray(files), timestamps=np.asarray(timestamps),\n",
    "    vocab=np.asarray(list(vocabulary.keys())), matrix=matrix\n",
    "  )\n",
    "\n",
    "thread = Thread(target=save_cache, args=(), daemon=True)\n",
    "thread.start()\n",
    "\n",
    "roots_and_filters = [\n",
    "  ['C:\\\\Users\\\\bkand\\\\Downloads\\\\', lambda x: x.endswith('pdf')],\n",
    "  ['E:\\\\imp\\\\', lambda x: True],\n",
    "  ['E:\\\\archivebox\\\\archive\\\\', lambda x: x.endswith('.html') and x.find('.orig')==-1 and x.find('woff2')==-1]\n",
    "]\n",
    "\n",
    "t1 = LoadFileNamesThread(roots_and_filters)\n",
    "t2 = LoadIndexThread()\n",
    "t1.start(); t2.start()\n",
    "t2.join(); t1.join()\n",
    "multi_threaded_time = time.time() - start\n",
    "\n",
    "start  = time.time()\n",
    "fts, v, m = load_cache()\n",
    "get_files()\n",
    "single_threaded_time = time.time() - start\n",
    "\n",
    "\n",
    "print(f'single - {single_threaded_time:.4f}\\nmulti - {multi_threaded_time:.4f}\\ndiff - {abs(single_threaded_time - multi_threaded_time):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def walk(base, filter=lambda x: x.endswith('pdf') or x.endswith('html')):\n",
    "  files = []\n",
    "  for p, d, f in os.walk(base):\n",
    "    for file in f:\n",
    "      if filter(file):\n",
    "        files.append(os.path.join(p, file))\n",
    "  return files  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = walk('C:\\\\Users\\\\bkand\\\\Downloads')\n",
    "# files = walk('E:\\\\archivebox\\\\archive')\n",
    "# extract_text_from_files(files[3])\n",
    "\n",
    "f = 'C:\\\\Users\\\\bkand\\\\Downloads\\\\Tainter_The_Collapse_of_Complex_Societies.pdf'\n",
    "\n",
    "\n",
    "# print(f)\n",
    "# with open(f, 'r', encoding='utf8') as html: \n",
    "#   soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "#   print(soup.find('title').text)\n",
    "#   print([p.text for p in soup.find_all('h1')])\n",
    "#   print([p.text for p in soup.find_all('h2')])\n",
    "\n",
    "\n",
    "# print(f)\n",
    "# doc = fitz.open(f)\n",
    "# print(f'author: {doc.metadata.get(\"author\")}\\ntitle: {doc.metadata[\"title\"]}')\n",
    "# doc[5].get_text() \n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "l = []\n",
    "for elem in l: \n",
    "  if elem % n:\n",
    "    l.remove(elem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46b9170fdf92c23cc3e3b131494cadcf9ba12dce192d70b41d5030bfbd5436a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
